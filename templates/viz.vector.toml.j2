[sources.thesink]
  # syslog from thesink via vector
  # General
  type = "vector" # Assuming the source is the Vector sink on 'thesink'
  address = "0.0.0.0:{{ hpviz_vector_port }}" # IP address of 'thesink'
  shutdown_timeout_secs = 30 #: 30s

  # TLS
  # Make sure to mount volume in docker-compose '/etc/ca-certificates/certificate_authority.crt:/etc/ssl/certs/certificate_authority.crt'
  tls.ca_file = "{{ local_ca_file }}" # Local to the vector container.
  tls.crt_file = "/etc/ssl/certs/{{ hpviz_vector_hostkey }}.crt" # as above
  tls.enabled = true
  tls.key_file = "/etc/ssl/private/{{ hpviz_vector_hostkey }}.key" # as above
  #tls.key_pass = " hpviz_vector_hostkey_secret " # TODO: Configure GITHUB SECRETS and enable 'hpviz_vector_hostkey_secret'
  tls.verify_certificate = false

[sources.syslog_viz001]
  # syslog from viz001 (localhost)
  type = "syslog"
  address = "0.0.0.0:{{ hpviz_syslog_ingest_port }}"
  mode = "tcp" # options 'udp', 'tcp', 'unix'
  # Context
  host_key = "host"

#===================================< tagging >=================================================#
[transforms.tag]
  #
  # First stage in processing pipeline
  #   everything tagged as 'syslog', 'json' and 'raw'
  #   as logs are processed the tag will be updated to 'processed' and also tagged as type i.e. 'sshd'
  #   anything not processed will remain with tag = 'raw'
  #
  type="add_fields"
  inputs = ["syslog_viz001", "thesink"]
  overwrite=true

  #tags
  fields.tags = ["syslog", "json", "raw"]

#===================================< transforms >===============================================#
# <<SSHD START>>

[transforms.regex_sshd_log]
  #
  #
  # This regex processes logs with a message of (examples):
  # Disconnected from 178.154.253.235 port 36640 [preauth]
  # Failed password for root from 37.152.180.201 port 34364 ssh2
  # Disconnected from invalid user maprdev 173.212.240.196 port 45598 [preauth]
  # Received disconnect from 49.235.107.161 port 35076:11: Bye Bye [preauth]
  # Disconnected from authenticating user root 51.77.150.118 port 56266 [preauth]
  # Connection closed by invalid user pi 109.116.77.115 port 39062 [preauth]

  type = "regex_parser"
  inputs = ["tag"]
  drop_failed = true
  drop_field = false
  field = "message"
  overwrite_target = true
  patterns = ['(?P<action>^(?:\w+\s?){1,5}.*) (?P<source.ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}) port (?P<source.port>[\d]+)']


[transforms.tag_regex_sshd_log]
  type="add_fields"
  inputs = ["regex_sshd_log"]
  overwrite=true

  #tags
  fields.tags = ["syslog", "json", "sshd"]

[transforms.sourceip_geoip_sshd_log]
  # GeoIP transform for source.ip
  type = "geoip"
  inputs = ["tag_regex_sshd_log"]
  database = "{{ hm_geoip__database_directory }}/GeoLite2-ASN.mmdb"
  source = "source.ip"
  target = "source"

[transforms.sourceip_geocity_sshd_log]
  # GeoCity transform for destination.ip
  type = "geoip"
  inputs = ["tag_regex_sshd_log"]
  database = "{{ hm_geoip__database_directory }}/GeoLite2-City.mmdb"
  source = "source.ip"
  target = "source"

[transforms.transform_sourceip_geoip_sshd_log]
  # json flattening transform for source.ip
  type = "json_parser"
  inputs = ["sourceip_geoip_sshd_log"]
  drop_field = true
  drop_invalid = false
  field = "source"
  overwrite_target = false #false
  #target_field = "src" # optional, no default


[transforms.transform_sourceip_geocity_sshd_log]
  # json flattening transform for src_ip
  type = "json_parser"
  inputs = ["sourceip_geocity_sshd_log"]
  drop_field = true
  drop_invalid = false
  field = "source"
  overwrite_target = false #false
  #target_field = "src" # optional, no default

[transforms.rename_transform_sourceip_geoip_sshd_log]
  # General
  type = "rename_fields"
  inputs = ["transform_sourceip_geoip_sshd_log"]

  # Fields
  fields.autonomous_system_number = "source.as.number"
  fields.autonomous_system_organization = "source.as.organisation.name"
  fields.isp = "source.isp.name"
  fields.organization = "source.organisation.name"


[transforms.rename_transform_sourceip_geocity_sshd_log]
  # General
  type = "rename_fields"
  inputs = ["transform_sourceip_geocity_sshd_log"]

  # Fields
  fields.city_name = "source.geo.city_name"
  fields.continent_code = "source.geo.continent_iso_code"
  fields.country_code = "source.geo.country_iso_code"
  fields.latitude = "source.geo.latitude"
  fields.longitude = "source.geo.longitude"
  fields.postal_code = "source.geo.postal_code"
  fields.timezone = "source.event.timezone"



[transforms.reduce_sshd_log]
  # General
  type = "reduce"
  inputs = ["rename_transform_sourceip_geoip_sshd_log", "rename_transform_sourceip_geocity_sshd_log"]
  #group_by = []

  expire_after_ms = 10
  flush_period_ms = 5

# SSHD Field = ["reduce_sshd_log"]

# <<SSHD END>>
#==================================================================================#

#==================================================================================#
# << COWRIE START >>

[transforms.regex_cowrie_log]
  # This regex processes logs with a message of (examples):
  # [SSHChannel cowrie-discarded-direct-tcpip (23) on SSHService 'ssh-connection' on HoneyPotSSHTransport,384039,5.182.39.61] discarded direct-tcp forward request 23 to 188.125.73.109:993 with data

  type = "regex_parser"
  inputs = ["tag"]
  drop_failed = true
  drop_field = false
  field = "message"
  overwrite_target = true
  patterns = ['(?P<process.name>[\w]+),(?P<process.pid>[\d]+),(?P<source.ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})]\s(?P<event.action>discarded direct-tcp forward request)\s[\d]+ to\s(?P<destination.ip>\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}+):(?P<destination.port>[0-9]+)']

[transforms.tag_regex_cowrie_log]
  type="add_fields"
  inputs = ["regex_cowrie_log"]
  overwrite=true

  #tags
  fields.tags = ["syslog", "json", "cowrie"]

[transforms.sourceip_geoip_cowrie_log]
  # GeoIP transform for source.ip
  type = "geoip"
  inputs = ["tag_regex_cowrie_log"]
  database = "{{ hm_geoip__database_directory }}/GeoLite2-ASN.mmdb"
  source = "source.ip"
  target = "source"

[transforms.destinationip_geoip_cowrie_log]
  # GeoIP transform for destination.ip
  type = "geoip"
  inputs = ["tag_regex_cowrie_log"]
  database = "{{ hm_geoip__database_directory }}/GeoLite2-ASN.mmdb"
  source = "destination.ip"
  target = "destination"

[transforms.sourceip_geocity_cowrie_log]
  # GeoCity transform for destination.ip
  type = "geoip"
  inputs = ["tag_regex_cowrie_log"]
  database = "{{ hm_geoip__database_directory }}/GeoLite2-City.mmdb"
  source = "source.ip"
  target = "source"

[transforms.destinationip_geocity_cowrie_log]
  # GeoCity transform for destination.ip
  type = "geoip"
  inputs = ["tag_regex_cowrie_log"]
  database = "{{ hm_geoip__database_directory }}/GeoLite2-City.mmdb"
  source = "destination.ip"
  target = "destination"

[transforms.transform_sourceip_geolite_cowrie_log]
  # json flattening transform for source.ip
  type = "json_parser"
  inputs = ["sourceip_geoip_cowrie_log", "sourceip_geocity_cowrie_log"]
  drop_field = true
  drop_invalid = false
  field = "source"
  overwrite_target = false #false
  #target_field = "src" # optional, no default

[transforms.transform_destinationip_geolite_cowrie_log]
  # json flattening transform for destination.ip
  type = "json_parser"
  inputs = ["destinationip_geoip_cowrie_log", "destinationip_geocity_cowrie_log"]
  drop_field = true
  drop_invalid = false
  field = "dst"
  overwrite_target = false #false
  #target_field = "dst"

[transforms.rename_transform_sourceip_geolite_cowrie_log]
  # General
  type = "rename_fields"
  inputs = ["transform_sourceip_geolite_cowrie_log"]

  # Fields
  fields.autonomous_system_number = "source.as.number"
  fields.autonomous_system_organization = "source.as.organisation.name"
  fields.isp = "source.isp.name"
  fields.organization = "source.organisation.name"
  fields.city_name = "source.geo.city_name"
  fields.continent_code = "source.geo.continent_iso_code"
  fields.country_code = "source.geo.country_iso_code"
  fields.latitude = "source.geo.latitude"
  fields.longitude = "source.geo.longitude"
  fields.postal_code = "source.geo.postal_code"
  fields.timezone = "source.event.timezone"

[transforms.rename_transform_destinationip_geolite_cowrie_log]
  # General
  type = "rename_fields"
  inputs = ["transform_destinationip_geolite_cowrie_log"]

  # Fields
  fields.autonomous_system_number = "destination.as.number"
  fields.autonomous_system_organization = "destination.as.organisation.name"
  fields.isp = "destination.isp.name"
  fields.organization = "destination.organisation.name"
  fields.city_name = "destination.geo.city_name"
  fields.continent_code = "destination.geo.continent_iso_code"
  fields.country_code = "destination.geo.country_iso_code"
  fields.latitude = "destination.geo.latitude"
  fields.longitude = "destination.geo.longitude"
  fields.postal_code = "destination.geo.postal_code"
  fields.timezone = "destination.event.timezone"

[transforms.reduce_cowrie_log]
  # General
  type = "reduce"
  inputs = ["rename_transform_sourceip_geolite_cowrie_log", "rename_transform_destinationip_geolite_cowrie_log"]
  # group_by = []

  expire_after_ms = 10
  flush_period_ms = 5


# << COWRIE END >>

#=================================< /transforms >=================================================#


# ==============================< Sinks >======================================== #

[sinks.kafka_processed]
  #
  # sink for all log messages that have been processed i.e. transformed and enriched with GeoIP
  #
  type = "kafka"
  inputs = ["reduce_cowrie_log", "reduce_sshd_log"]
  #inputs = ["rename_transform_src_ip_geolite_log", "rename_transform_dst_ip_geolite_log"]
  #inputs = ["regex_cowrie_1", "regex_sshd_1"]
  bootstrap_servers = "{{ hpviz_broker_bootstrap_server1 }}:{{ hpviz_broker_bootstrap_port1 }}" # IP required not hostname
  compression = "none"
  healthcheck = true
  key_field = "host"
  message_timeout_ms = 300000
  socket_timeout_ms = 60000
  topic = "logging.syslog.processed"

  # Buffer
  buffer.max_events = 500
  buffer.max_size = 104900000
  buffer.type = "memory"
  buffer.when_full = "block"

  # Encoding
  encoding.codec = "json"
  encoding.timestamp_format = "rfc3339"


[sinks.kafka_raw]
  #
  # sink for raw unprocessed logs, i.e. these logs have not been transformed or enriched
  #
  type = "kafka"
  inputs = ["tag"]
  bootstrap_servers = "{{ hpviz_broker_bootstrap_server1 }}:{{ hpviz_broker_bootstrap_port1 }}" # IP required not hostname
  compression = "none"
  healthcheck = true
  key_field = "host"
  message_timeout_ms = 300000
  socket_timeout_ms = 60000
  topic = "logging.syslog.raw"

  # Buffer
  buffer.max_events = 500
  buffer.max_size = 104900000
  buffer.type = "memory"
  buffer.when_full = "block"

  # Encoding
  encoding.codec = "json"
  encoding.timestamp_format = "rfc3339"
